# NLP Review Sentiment Classification

ğŸ† **1st Place â€“ Public Leaderboard**  
ğŸ¥‰ **3rd Place â€“ Private Leaderboard**  
*Kaggle Neural Networks Competition â€“ Faculty of Computer and Information Sciences, Ain Shams University*

---

## ğŸ“Œ Project Overview

This project is an **NLP Review Sentiment Classification system**.

The goal of this project is to help websites understand their overall ranking based on usersâ€™ reviews.  
By developing deep learning models, we automatically convert a userâ€™s written comment into one of **five sentiment classes**:

**(Excellent, Very Good, Good, Bad, Very Bad)**

The system is designed to accurately classify each review while handling class imbalance and real-world noisy text.

---

## ğŸ§  Why This Project?

User reviews are subjective, noisy, and often imbalanced.  
This project focuses on:
- Understanding **contextual meaning**
- Handling **ordinal sentiment levels**
- Improving performance on **minority classes**
- Building **robust ensemble models**

---

## ğŸ”¹ Text Preprocessing & Feature Engineering

To improve model generalization and reduce noise, the following preprocessing steps were applied:

- Lowercasing text
- Removing URLs and HTML tags
- Removing special characters and extra spaces
- Expanding contractions (e.g., *can't â†’ cannot*)
- Text normalization

ğŸ“Œ **Why?**  
Clean and normalized text helps models focus on semantic meaning instead of irrelevant noise.

---

## ğŸ”¹ Data Augmentation & Class Balancing

The dataset was imbalanced, so minority classes were augmented using:
- Synonym replacement
- Word deletion
- Word swapping
- Word insertion

Additionally:
- Class weights
- Balanced Accuracy metric
- Weighted sampling

ğŸ“Œ **Why?**  
To prevent the model from being biased toward majority classes and to improve performance across all sentiment levels.

---

## ğŸ”¹ Models Used

### ğŸ”¹ 1. Transformer-Based Model (DeBERTa â€“ PyTorch)

- Fine-tuned **DeBERTa-v3 Large**
- Disentangled attention mechanism
- Strong contextual understanding
- Layer-wise learning rate decay (LLRD)
- Custom **Ordinal + Focal Loss**

ğŸ“Œ **Why DeBERTa?**  
It performs exceptionally well on sentiment and contextual NLP tasks.

ğŸ“Œ **Why Ordinal + Focal Loss?**
- Focal Loss focuses on hard samples
- Ordinal Loss respects sentiment order (Very Bad â†’ Excellent)

---

### ğŸ”¹ 2. Deep Learning Models from Scratch (TensorFlow)

Multiple architectures were trained to capture different linguistic patterns:

- **Transformer Encoder**  
  â†’ Captures global context

- **BiLSTM**  
  â†’ Learns sequential and bidirectional dependencies

- **CNN + BiLSTM**  
  â†’ Extracts local n-gram features + sequence modeling

- **BiGRU with Attention**  
  â†’ Focuses on the most important words in each review

ğŸ“Œ **Why multiple models?**  
Each architecture captures different aspects of language.

---

## ğŸ”¹ Ensemble Learning

Final predictions were generated using a **weighted ensemble**, where:
- Each model was weighted based on its validation **Balanced Accuracy**
- Probabilities were combined to produce final predictions

ğŸ“Œ **Why ensemble?**  
Ensembles improve stability, robustness, and overall performance.

---

## ğŸ“Š Evaluation Metric

- **Balanced Accuracy**
  
Chosen due to class imbalance, ensuring fair evaluation across all classes.

---

## ğŸ›  Tech Stack

- **Python**
- **PyTorch**
- **TensorFlow / Keras**
- **HuggingFace Transformers**
- **Scikit-learn**
- **NumPy & Pandas**

---

## ğŸš€ Key Learnings

This project strengthened my experience in:
- NLP pipelines
- Transformer fine-tuning
- Sequence models (LSTM, GRU)
- Attention mechanisms
- Ensemble learning
- Handling real-world imbalanced datasets
- Working with both PyTorch and TensorFlow

---

## ğŸ“Œ Competition Achievement

ğŸ¥‡ **1st Place â€“ Public Leaderboard**  
ğŸ¥‰ **3rd Place â€“ Private Leaderboard**

---
