{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":121535,"databundleVersionId":14530012,"sourceType":"competition"},{"sourceId":14098595,"sourceType":"datasetVersion","datasetId":8978579},{"sourceId":14134091,"sourceType":"datasetVersion","datasetId":9006504},{"sourceId":14134174,"sourceType":"datasetVersion","datasetId":9006563},{"sourceId":14140402,"sourceType":"datasetVersion","datasetId":9011301}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport random\nimport warnings\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom transformers import (\n    AutoTokenizer, \n    AutoModel, \n    AutoConfig,\n    get_cosine_schedule_with_warmup,\n    DebertaV2Config,\n    DebertaV2Model,\n    PreTrainedTokenizerFast\n)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom safetensors.torch import load_file\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nCONFIG = {\n    'train_path': '/kaggle/input/nn-26-review-sentiment-classification/train.csv',\n    'test_path': '/kaggle/input/nn-26-review-sentiment-classification/test.csv',\n    'save_dir': '/kaggle/working/',\n    'transformer_model': 'microsoft/deberta-v3-large',\n    'pt_max_len': 384,\n    'pt_batch_size': 4,\n    'pt_accumulation_steps': 8,\n    'val_size': 0.15,\n    'pt_epochs': 6,\n    'pt_learning_rate': 2e-5,\n    'pt_weight_decay': 0.01,\n    'pt_warmup_ratio': 0.1,\n    'pt_dropout': 0.3,\n    'llrd_decay': 0.95,\n    'tf_max_features': 30000,\n    'tf_max_len': 200,\n    'tf_embedding_dim': 300,\n    'tf_batch_size': 16,  # REDUCED from 32\n    'tf_epochs': 30,\n    'tf_learning_rate': 0.001,\n    'tf_lstm_units': 128,\n    'tf_gru_units': 128,\n    'tf_cnn_filters': 128,\n    'tf_dropout_rate': 0.4,\n    'transformer_heads': 8,\n    'transformer_ff_dim': 512,\n    'transformer_blocks': 2,  # REDUCED from 4\n    'label_smoothing': 0.1,\n    'focal_gamma': 2.0,\n    'ordinal_alpha': 1.5,\n    'augment_minority': True,\n    'target_samples': 2500,\n    'seed': 42,\n}\n\nCLASS_LABELS = ['Very bad', 'Bad', 'Good', 'Very good', 'Excellent']\nLABEL_TO_IDX = {label: idx for idx, label in enumerate(CLASS_LABELS)}\nIDX_TO_LABEL = {idx: label for idx, label in enumerate(CLASS_LABELS)}\nNUM_CLASSES = len(CLASS_LABELS)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(CONFIG['seed'])\n\ndef clean_text(text):\n    if pd.isna(text):\n        return \"\"\n    text = str(text)\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'<.*?>', '', text)\n    contractions = {\n        \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\",\n        \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n    }\n    for contraction, expansion in contractions.items():\n        text = text.replace(contraction, expansion)\n    text = re.sub(r'[^a-zA-Z0-9\\s!?.,;:\\'-]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef get_layerwise_optimizer(model, base_lr, weight_decay, llrd_decay=0.95):\n    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n    n_layers = model.config.num_hidden_layers\n    optimizer_grouped_parameters = []\n    \n    optimizer_grouped_parameters.extend([\n        {\n            'params': [p for n, p in model.classifier.named_parameters() \n                      if not any(nd in n for nd in no_decay)],\n            'weight_decay': weight_decay,\n            'lr': base_lr\n        },\n        {\n            'params': [p for n, p in model.classifier.named_parameters() \n                      if any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0,\n            'lr': base_lr\n        }\n    ])\n    \n    if hasattr(model, 'layer_norm'):\n        optimizer_grouped_parameters.append({\n            'params': model.layer_norm.parameters(),\n            'weight_decay': 0.0,\n            'lr': base_lr * 0.95\n        })\n    \n    for layer_idx in range(n_layers - 1, -1, -1):\n        layer_lr = base_lr * (llrd_decay ** (n_layers - layer_idx))\n        layer = model.transformer.encoder.layer[layer_idx]\n        \n        optimizer_grouped_parameters.extend([\n            {\n                'params': [p for n, p in layer.named_parameters() \n                          if not any(nd in n for nd in no_decay)],\n                'weight_decay': weight_decay,\n                'lr': layer_lr\n            },\n            {\n                'params': [p for n, p in layer.named_parameters() \n                          if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': layer_lr\n            }\n        ])\n    \n    if hasattr(model.transformer, 'embeddings'):\n        emb_lr = base_lr * (llrd_decay ** (n_layers + 1))\n        optimizer_grouped_parameters.extend([\n            {\n                'params': [p for n, p in model.transformer.embeddings.named_parameters() \n                          if not any(nd in n for nd in no_decay)],\n                'weight_decay': weight_decay,\n                'lr': emb_lr\n            },\n            {\n                'params': [p for n, p in model.transformer.embeddings.named_parameters() \n                          if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': emb_lr\n            }\n        ])\n    \n    return AdamW(optimizer_grouped_parameters)\n\nclass TextAugmenter:\n    def __init__(self):\n        self.synonyms = {\n            'good': ['great', 'nice', 'fine', 'excellent', 'wonderful', 'fantastic'],\n            'bad': ['terrible', 'awful', 'poor', 'horrible', 'dreadful', 'lousy'],\n            # ... (rest of synonyms)\n        }\n    \n    def synonym_replacement(self, text, n=2):\n        words = text.split()\n        new_words = words.copy()\n        replacements = 0\n        indices = list(range(len(words)))\n        random.shuffle(indices)\n        \n        for i in indices:\n            word_lower = words[i].lower()\n            if word_lower in self.synonyms and replacements < n:\n                synonym = random.choice(self.synonyms[word_lower])\n                new_words[i] = synonym\n                replacements += 1\n        \n        return ' '.join(new_words)\n    \n    def augment(self, text):\n        return self.synonym_replacement(text, n=random.randint(1, 3))\n\ndef augment_to_target(df, text_col, label_col, target_samples=2500):\n    augmenter = TextAugmenter()\n    class_counts = df[label_col].value_counts()\n    print(\"\\nOriginal class distribution:\")\n    print(class_counts)\n    \n    augmented_data = []\n    for label in class_counts.index:\n        label_data = df[df[label_col] == label]\n        current_count = len(label_data)\n        samples_needed = target_samples - current_count\n        \n        if samples_needed > 0:\n            print(f\"Augmenting '{label}': {current_count} → {target_samples} (+{samples_needed})\")\n            for _ in range(samples_needed):\n                sample = label_data.sample(1).iloc[0]\n                aug_text = augmenter.augment(sample[text_col])\n                augmented_data.append({\n                    text_col: aug_text,\n                    label_col: label,\n                    'id': -1\n                })\n    \n    if augmented_data:\n        aug_df = pd.DataFrame(augmented_data)\n        df = pd.concat([df, aug_df], ignore_index=True)\n    \n    return df\n\nclass CombinedOrdinalFocalLoss(nn.Module):\n    def __init__(self, alpha=None, focal_gamma=2.0, ordinal_alpha=1.5, \n                 focal_weight=0.7, label_smoothing=0.1):\n        super().__init__()\n        self.alpha = alpha\n        self.focal_gamma = focal_gamma\n        self.ordinal_alpha = ordinal_alpha\n        self.focal_weight = focal_weight\n        self.ordinal_weight = 1.0 - focal_weight\n        self.label_smoothing = label_smoothing\n    \n    def focal_component(self, logits, targets):\n        probs = F.softmax(logits, dim=-1)\n        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n        focal_weight = (1 - pt) ** self.focal_gamma\n        \n        ce_loss = F.cross_entropy(\n            logits, targets, \n            reduction='none',\n            label_smoothing=self.label_smoothing\n        )\n        \n        if self.alpha is not None:\n            alpha_t = self.alpha.gather(0, targets)\n            focal_loss = alpha_t * focal_weight * ce_loss\n        else:\n            focal_loss = focal_weight * ce_loss\n        \n        return focal_loss.mean()\n    \n    def ordinal_component(self, logits, targets):\n        probs = F.softmax(logits, dim=-1)\n        num_classes = logits.size(1)\n        \n        ordinal_loss = 0\n        for i in range(num_classes):\n            distance = torch.abs(i - targets).float()\n            ordinal_loss += -torch.log(1 - probs[:, i] + 1e-8) * (distance ** self.ordinal_alpha)\n        \n        return ordinal_loss.mean()\n    \n    def forward(self, logits, targets):\n        focal = self.focal_component(logits, targets)\n        ordinal = self.ordinal_component(logits, targets)\n        return self.focal_weight * focal + self.ordinal_weight * ordinal\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\nclass DeBERTaClassifier(nn.Module):\n    def __init__(self, transformer_model, num_classes, dropout=0.3):\n        super().__init__()\n        \n        self.config = DebertaV2Config(\n            architectures=[\"DebertaV2Model\"],\n            attention_probs_dropout_prob=0.1,\n            hidden_act=\"gelu\",\n            hidden_dropout_prob=0.1,\n            hidden_size=1024,\n            initializer_range=0.02,\n            intermediate_size=4096,\n            layer_norm_eps=1e-7,\n            legacy=True,\n            max_position_embeddings=512,\n            max_relative_positions=-1,\n            model_type=\"deberta-v2\",\n            norm_rel_ebd=\"layer_norm\",\n            num_attention_heads=16,\n            num_hidden_layers=24,\n            pad_token_id=0,\n            pooler_dropout=0,\n            pooler_hidden_act=\"gelu\",\n            pooler_hidden_size=1024,\n            pos_att_type=[\"p2c\", \"c2p\"],\n            position_biased_input=False,\n            position_buckets=256,\n            relative_attention=True,\n            share_att_key=True,\n            type_vocab_size=0,\n            vocab_size=128100,\n            torch_dtype=\"float32\"\n        )\n        \n        self.transformer = DebertaV2Model(self.config)\n        state_dict = load_file(\"/kaggle/input/deberta-dataset/out/model.safetensors\")\n        self.transformer.load_state_dict(state_dict)\n        \n        hidden_size = self.config.hidden_size\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for _ in range(5)])\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, num_classes)\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n            pooled = outputs.pooler_output\n        else:\n            last_hidden = outputs.last_hidden_state\n            attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n            sum_hidden = torch.sum(last_hidden * attention_mask_expanded, 1)\n            sum_mask = attention_mask_expanded.sum(1).clamp(min=1e-9)\n            pooled = sum_hidden / sum_mask\n        \n        pooled = self.layer_norm(pooled)\n        \n        logits = torch.zeros(pooled.size(0), self.classifier[-1].out_features).to(pooled.device)\n        for dropout in self.dropouts:\n            logits += self.classifier(dropout(pooled))\n        logits /= len(self.dropouts)\n        \n        return logits\n\ndef train_epoch_pytorch(model, dataloader, optimizer, scheduler, criterion, device, accumulation_steps):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    optimizer.zero_grad()\n    \n    for step, batch in enumerate(dataloader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss = loss / accumulation_steps\n        \n        loss.backward()\n        \n        if (step + 1) % accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        total_loss += loss.item() * accumulation_steps\n        \n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = balanced_accuracy_score(all_labels, all_preds)\n    \n    return avg_loss, accuracy\n\ndef validate_pytorch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = balanced_accuracy_score(all_labels, all_preds)\n    \n    return avg_loss, accuracy, all_preds\n\ndef predict_pytorch(model, dataloader, device):\n    model.eval()\n    all_probs = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            probs = F.softmax(outputs, dim=1)\n            all_probs.extend(probs.cpu().numpy())\n    \n    return np.array(all_probs)\n\n# TRAIN DEBERTA MODEL\nprint(\"=\" * 80)\nprint(\"TRAINING DeBERTa TRANSFORMER\")\nprint(\"=\" * 80)\n\nos.makedirs(CONFIG['save_dir'], exist_ok=True)\n\nprint(\"\\n[1/2] Loading and preprocessing data...\")\ntrain_df = pd.read_csv(CONFIG['train_path'])\ntest_df = pd.read_csv(CONFIG['test_path'])\n\ntrain_df['cleaned_text'] = train_df['text'].apply(clean_text)\ntest_df['cleaned_text'] = test_df['text'].apply(clean_text)\n\nle = LabelEncoder()\nle.fit(CLASS_LABELS)\ntrain_df['label_idx'] = le.transform(train_df['review'])\n\nif CONFIG['augment_minority']:\n    print(\"\\n[2/2] Augmenting data...\")\n    train_df = augment_to_target(train_df, 'cleaned_text', 'review', CONFIG['target_samples'])\n    train_df['label_idx'] = le.transform(train_df['review'])\n\nclass_counts = np.bincount(train_df['label_idx'].values, minlength=NUM_CLASSES)\nclass_weights = 1.0 / (class_counts + 1e-6)\nclass_weights = class_weights / class_weights.sum() * NUM_CLASSES\nclass_weights_pt = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n\nnp.save(os.path.join(CONFIG['save_dir'], 'label_classes.npy'), le.classes_)\n\ntokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/kaggle/input/deberta-dataset/out/tokenizer.json\")\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.save_pretrained(CONFIG['save_dir'])\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df['cleaned_text'].values,\n    train_df['label_idx'].values,\n    test_size=CONFIG['val_size'],\n    stratify=train_df['label_idx'].values,\n    random_state=CONFIG['seed']\n)\n\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer, CONFIG['pt_max_len'])\nval_dataset = TextDataset(val_texts, val_labels, tokenizer, CONFIG['pt_max_len'])\ntest_dataset = TextDataset(\n    test_df['cleaned_text'].values,\n    np.zeros(len(test_df)),\n    tokenizer,\n    CONFIG['pt_max_len']\n)\n\nsample_weights = class_weights_pt[train_labels].cpu().numpy()\nsampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG['pt_batch_size'], sampler=sampler)\nval_loader = DataLoader(val_dataset, batch_size=CONFIG['pt_batch_size'] * 2, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['pt_batch_size'] * 2, shuffle=False)\n\nmodel = DeBERTaClassifier(\n    CONFIG['transformer_model'],\n    NUM_CLASSES,\n    dropout=CONFIG['pt_dropout']\n).to(DEVICE)\n\noptimizer = get_layerwise_optimizer(\n    model, \n    base_lr=CONFIG['pt_learning_rate'],\n    weight_decay=CONFIG['pt_weight_decay'],\n    llrd_decay=CONFIG['llrd_decay']\n)\n\ntotal_steps = len(train_loader) * CONFIG['pt_epochs'] // CONFIG['pt_accumulation_steps']\nwarmup_steps = int(total_steps * CONFIG['pt_warmup_ratio'])\nscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\ncriterion = CombinedOrdinalFocalLoss(\n    alpha=class_weights_pt,\n    focal_gamma=CONFIG['focal_gamma'],\n    ordinal_alpha=CONFIG['ordinal_alpha'],\n    focal_weight=0.7,\n    label_smoothing=CONFIG['label_smoothing']\n)\n\nbest_val_acc = 0\npatience = 3\npatience_counter = 0\n\nprint(\"\\nStarting training...\")\nfor epoch in range(CONFIG['pt_epochs']):\n    train_loss, train_acc = train_epoch_pytorch(\n        model, train_loader, optimizer, scheduler,\n        criterion, DEVICE, CONFIG['pt_accumulation_steps']\n    )\n    \n    val_loss, val_acc, _ = validate_pytorch(model, val_loader, criterion, DEVICE)\n    \n    print(f\"Epoch {epoch+1}/{CONFIG['pt_epochs']} | \"\n          f\"Train Loss: {train_loss:.4f} | Train BA: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} | Val BA: {val_acc:.4f}\")\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        patience_counter = 0\n        torch.save(model.state_dict(), \n                  os.path.join(CONFIG['save_dir'], 'best_deberta.pt'))\n        print(f\"  ✓ New best model saved! BA: {val_acc:.4f}\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"  ⚠ Early stopping!\")\n            break\n\nmodel.load_state_dict(torch.load(os.path.join(CONFIG['save_dir'], 'best_deberta.pt')))\ntest_probs_deberta = predict_pytorch(model, test_loader, DEVICE)\n\nprint(f\"\\n✓ DeBERTa Best Val BA: {best_val_acc:.4f}\")\n\n# Save DeBERTa predictions\nnp.save(os.path.join(CONFIG['save_dir'], 'deberta_probs.npy'), test_probs_deberta)\n\n# FREE GPU MEMORY - CRITICAL!\ndel model, optimizer, scheduler, criterion\ndel train_loader, val_loader, test_loader\ndel train_dataset, val_dataset, test_dataset\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()\n\nprint(\"\\n✓ GPU memory cleared for TensorFlow models\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T07:39:42.727767Z","iopub.execute_input":"2025-12-13T07:39:42.728077Z","iopub.status.idle":"2025-12-13T10:51:32.099945Z","shell.execute_reply.started":"2025-12-13T07:39:42.728054Z","shell.execute_reply":"2025-12-13T10:51:32.099110Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\nUsing device: cuda\n================================================================================\nTRAINING DeBERTa TRANSFORMER\n================================================================================\n\n[1/2] Loading and preprocessing data...\n\n[2/2] Augmenting data...\n\nOriginal class distribution:\nreview\nVery good    2469\nExcellent    2335\nGood         1024\nBad           648\nVery bad      524\nName: count, dtype: int64\nAugmenting 'Very good': 2469 → 2500 (+31)\nAugmenting 'Excellent': 2335 → 2500 (+165)\nAugmenting 'Good': 1024 → 2500 (+1476)\nAugmenting 'Bad': 648 → 2500 (+1852)\nAugmenting 'Very bad': 524 → 2500 (+1976)\n\nStarting training...\nEpoch 1/6 | Train Loss: 1.1051 | Train BA: 0.5532 | Val Loss: 0.8604 | Val BA: 0.7179\n  ✓ New best model saved! BA: 0.7179\nEpoch 2/6 | Train Loss: 0.6525 | Train BA: 0.7991 | Val Loss: 0.7625 | Val BA: 0.7824\n  ✓ New best model saved! BA: 0.7824\nEpoch 3/6 | Train Loss: 0.4422 | Train BA: 0.8776 | Val Loss: 0.6465 | Val BA: 0.8256\n  ✓ New best model saved! BA: 0.8256\nEpoch 4/6 | Train Loss: 0.2935 | Train BA: 0.9259 | Val Loss: 0.8292 | Val BA: 0.8293\n  ✓ New best model saved! BA: 0.8293\nEpoch 5/6 | Train Loss: 0.2181 | Train BA: 0.9510 | Val Loss: 0.8528 | Val BA: 0.8400\n  ✓ New best model saved! BA: 0.8400\nEpoch 6/6 | Train Loss: 0.1674 | Train BA: 0.9655 | Val Loss: 0.8955 | Val BA: 0.8427\n  ✓ New best model saved! BA: 0.8427\n\n✓ DeBERTa Best Val BA: 0.8427\n\n✓ GPU memory cleared for TensorFlow models\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport warnings\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Set memory growth for TensorFlow to prevent OOM\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    try:\n        for device in physical_devices:\n            tf.config.experimental.set_memory_growth(device, True)\n    except:\n        pass\n\nprint(f\"TensorFlow version: {tf.__version__}\")\n\n# Use existing CONFIG from previous cell\nCLASS_LABELS = ['Very bad', 'Bad', 'Good', 'Very good', 'Excellent']\nLABEL_TO_IDX = {label: idx for idx, label in enumerate(CLASS_LABELS)}\nIDX_TO_LABEL = {idx: label for idx, label in enumerate(CLASS_LABELS)}\nNUM_CLASSES = len(CLASS_LABELS)\n\nCONFIG = {\n    'train_path': '/kaggle/input/nn-26-review-sentiment-classification/train.csv',\n    'test_path': '/kaggle/input/nn-26-review-sentiment-classification/test.csv',\n    'save_dir': '/kaggle/working',\n    'tf_max_features': 30000,\n    'tf_max_len': 200,\n    'tf_embedding_dim': 200,  # REDUCED from 300\n    'tf_batch_size': 16,  # REDUCED from 32\n    'tf_epochs': 30,\n    'tf_learning_rate': 0.001,\n    'tf_lstm_units': 96,  # REDUCED from 128\n    'tf_gru_units': 96,  # REDUCED from 128\n    'tf_cnn_filters': 96,  # REDUCED from 128\n    'tf_dropout_rate': 0.4,\n    'transformer_heads': 8,\n    'transformer_ff_dim': 256,  # REDUCED from 512\n    'transformer_blocks': 2,  # REDUCED from 4\n    'seed': 42,\n}\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(CONFIG['seed'])\n\ndef clean_text(text):\n    if pd.isna(text):\n        return \"\"\n    text = str(text)\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'<.*?>', '', text)\n    contractions = {\n        \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\",\n        \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n    }\n    for contraction, expansion in contractions.items():\n        text = text.replace(contraction, expansion)\n    text = re.sub(r'[^a-zA-Z0-9\\s!?.,;:\\'-]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\nclass BahdanauAttention(layers.Layer):\n    def __init__(self, units, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.W1 = layers.Dense(units)\n        self.W2 = layers.Dense(units)\n        self.V = layers.Dense(1)\n    \n    def call(self, query, values):\n        query_with_time_axis = tf.expand_dims(query, 1)\n        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\"units\": self.units})\n        return config\n\nclass PositionalEncoding(layers.Layer):\n    def __init__(self, max_len, d_model, **kwargs):\n        super().__init__(**kwargs)\n        self.max_len = max_len\n        self.d_model = d_model\n        \n        position = np.arange(max_len)[:, np.newaxis]\n        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n        \n        pos_encoding = np.zeros((max_len, d_model))\n        pos_encoding[:, 0::2] = np.sin(position * div_term)\n        pos_encoding[:, 1::2] = np.cos(position * div_term)\n        \n        self.pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n    \n    def call(self, x):\n        seq_len = tf.shape(x)[1]\n        return x + self.pos_encoding[:seq_len, :]\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\"max_len\": self.max_len, \"d_model\": self.d_model})\n        return config\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.ff_dim = ff_dim\n        self.dropout_rate = dropout_rate\n        \n        self.att = layers.MultiHeadAttention(\n            num_heads=num_heads, \n            key_dim=embed_dim // num_heads,\n            dropout=dropout_rate\n        )\n        self.ffn = keras.Sequential([\n            layers.Dense(ff_dim, activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dim),\n        ])\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(dropout_rate)\n        self.dropout2 = layers.Dropout(dropout_rate)\n    \n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs, training=training)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        \n        ffn_output = self.ffn(out1, training=training)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n        \n        return out2\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"ff_dim\": self.ff_dim,\n            \"dropout_rate\": self.dropout_rate\n        })\n        return config\n\ndef create_transformer_model(vocab_size, num_classes=5):\n    inputs = layers.Input(shape=(CONFIG['tf_max_len'],))\n    \n    x = layers.Embedding(vocab_size, CONFIG['tf_embedding_dim'])(inputs)\n    x = layers.SpatialDropout1D(0.2)(x)\n    \n    x = PositionalEncoding(CONFIG['tf_max_len'], CONFIG['tf_embedding_dim'])(x)\n    \n    for _ in range(CONFIG['transformer_blocks']):\n        x = TransformerBlock(\n            embed_dim=CONFIG['tf_embedding_dim'],\n            num_heads=CONFIG['transformer_heads'],\n            ff_dim=CONFIG['transformer_ff_dim'],\n            dropout_rate=CONFIG['tf_dropout_rate'] * 0.5\n        )(x)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool, max_pool])\n    \n    x = layers.Dense(256, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    x = layers.Dense(128, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    \n    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    return keras.Model(inputs=inputs, outputs=outputs, name='Transformer')\n\ndef create_bilstm_model(vocab_size, num_classes=5):\n    inputs = layers.Input(shape=(CONFIG['tf_max_len'],))\n    \n    x = layers.Embedding(vocab_size, CONFIG['tf_embedding_dim'], mask_zero=True)(inputs)\n    x = layers.SpatialDropout1D(0.2)(x)\n    \n    x = layers.Bidirectional(\n        layers.LSTM(CONFIG['tf_lstm_units'], return_sequences=True, dropout=0.2, recurrent_dropout=0)\n    )(x)\n    x = layers.Bidirectional(\n        layers.LSTM(CONFIG['tf_lstm_units'] // 2, return_sequences=True, dropout=0.2, recurrent_dropout=0)\n    )(x)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool, max_pool])\n    \n    x = layers.Dense(256, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    x = layers.Dense(128, activation='gelu')(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    \n    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    return keras.Model(inputs=inputs, outputs=outputs, name='BiLSTM')\n\ndef create_cnn_bilstm_model(vocab_size, num_classes=5):\n    inputs = layers.Input(shape=(CONFIG['tf_max_len'],))\n    \n    x = layers.Embedding(vocab_size, CONFIG['tf_embedding_dim'])(inputs)\n    x = layers.SpatialDropout1D(0.2)(x)\n    \n    conv_blocks = []\n    for kernel_size in [2, 3, 4]:  # REDUCED from [2,3,4,5]\n        conv = layers.Conv1D(CONFIG['tf_cnn_filters'], kernel_size, activation='relu', padding='same')(x)\n        conv = layers.BatchNormalization()(conv)\n        conv = layers.GlobalMaxPooling1D()(conv)\n        conv_blocks.append(conv)\n    \n    cnn_features = layers.concatenate(conv_blocks)\n    \n    lstm_out = layers.Bidirectional(\n        layers.LSTM(CONFIG['tf_lstm_units'], return_sequences=True, recurrent_dropout=0)\n    )(x)\n    lstm_out = layers.GlobalAveragePooling1D()(lstm_out)\n    \n    x = layers.concatenate([cnn_features, lstm_out])\n    \n    x = layers.Dense(256, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    x = layers.Dense(128, activation='gelu')(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    \n    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    return keras.Model(inputs=inputs, outputs=outputs, name='CNN_BiLSTM')\n\ndef create_gru_attention_model(vocab_size, num_classes=5):\n    inputs = layers.Input(shape=(CONFIG['tf_max_len'],))\n    \n    x = layers.Embedding(vocab_size, CONFIG['tf_embedding_dim'], mask_zero=True)(inputs)\n    x = layers.SpatialDropout1D(0.2)(x)\n    \n    x = layers.Bidirectional(\n        layers.GRU(CONFIG['tf_gru_units'], return_sequences=True, dropout=0.2, recurrent_dropout=0)\n    )(x)\n    gru_out = layers.Bidirectional(\n        layers.GRU(CONFIG['tf_gru_units'] // 2, return_sequences=True, dropout=0.2, recurrent_dropout=0)\n    )(x)\n    \n    query = layers.Lambda(lambda x: x[:, -1, :])(gru_out)\n    context_vector, _ = BahdanauAttention(CONFIG['tf_gru_units'])(query, gru_out)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(gru_out)\n    max_pool = layers.GlobalMaxPooling1D()(gru_out)\n    \n    x = layers.concatenate([context_vector, avg_pool, max_pool])\n    \n    x = layers.Dense(256, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    x = layers.Dense(128, activation='gelu')(x)\n    \n    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    return keras.Model(inputs=inputs, outputs=outputs, name='GRU_Attention')\n\nclass BalancedAccuracyCallback(keras.callbacks.Callback):\n    def __init__(self, validation_data):\n        super().__init__()\n        self.validation_data = validation_data\n        self.best_ba = 0.0\n    \n    def on_epoch_end(self, epoch, logs=None):\n        X_val, y_val = [], []\n        for x, y in self.validation_data:\n            X_val.append(x.numpy())\n            y_val.append(y.numpy())\n        X_val = np.concatenate(X_val)\n        y_val = np.concatenate(y_val)\n        \n        y_pred = self.model.predict(X_val, verbose=0)\n        y_pred_classes = np.argmax(y_pred, axis=1)\n        \n        ba = balanced_accuracy_score(y_val, y_pred_classes)\n        logs['val_balanced_accuracy'] = ba\n        \n        if ba > self.best_ba:\n            self.best_ba = ba\n            print(f\" - val_balanced_accuracy: {ba:.4f} (best)\")\n\n# TRAIN TENSORFLOW MODELS\nprint(\"=\" * 80)\nprint(\"TRAINING TensorFlow MODELS\")\nprint(\"=\" * 80)\n\n# Load data\ntrain_df = pd.read_csv(CONFIG['train_path'])\ntest_df = pd.read_csv(CONFIG['test_path'])\n\ntrain_df['cleaned_text'] = train_df['text'].apply(clean_text)\ntest_df['cleaned_text'] = test_df['text'].apply(clean_text)\n\nle = LabelEncoder()\nle.fit(CLASS_LABELS)\ntrain_df['label_idx'] = le.transform(train_df['review'])\n\n# Load augmented data if exists\naugmented_path = os.path.join(CONFIG['save_dir'], 'augmented_train.csv')\nif os.path.exists(augmented_path):\n    train_df = pd.read_csv(augmented_path)\n    train_df['label_idx'] = le.transform(train_df['review'])\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train_df['cleaned_text'].values,\n    train_df['label_idx'].values,\n    test_size=0.15,\n    stratify=train_df['label_idx'].values,\n    random_state=CONFIG['seed']\n)\n\nprint(\"\\nCreating text vectorization...\")\nvectorize_layer = layers.TextVectorization(\n    max_tokens=CONFIG['tf_max_features'],\n    output_mode='int',\n    output_sequence_length=CONFIG['tf_max_len'],\n)\nvectorize_layer.adapt(np.concatenate([X_train, X_val]))\nvocab_size = len(vectorize_layer.get_vocabulary())\nprint(f\"Vocabulary size: {vocab_size}\")\n\nX_train_vec = vectorize_layer(X_train).numpy()\nX_val_vec = vectorize_layer(X_val).numpy()\nX_test_vec = vectorize_layer(test_df['cleaned_text'].values).numpy()\n\nconfig_data = {\n    'vocab': vectorize_layer.get_vocabulary(),\n    'max_len': CONFIG['tf_max_len'],\n    'idx_to_label': IDX_TO_LABEL,\n    'label_to_idx': LABEL_TO_IDX\n}\nwith open(os.path.join(CONFIG['save_dir'], 'tf_config.pkl'), 'wb') as f:\n    pickle.dump(config_data, f)\n\ntf_class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\ntf_class_weights_dict = {i: w for i, w in enumerate(tf_class_weights)}\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train_vec, y_train))\ntrain_ds = train_ds.shuffle(len(X_train)).batch(CONFIG['tf_batch_size']).prefetch(tf.data.AUTOTUNE)\n\nval_ds = tf.data.Dataset.from_tensor_slices((X_val_vec, y_val))\nval_ds = val_ds.batch(CONFIG['tf_batch_size']).prefetch(tf.data.AUTOTUNE)\n\ntf_models = [\n    ('Transformer', create_transformer_model),\n    ('BiLSTM', create_bilstm_model),\n    ('CNN_BiLSTM', create_cnn_bilstm_model),\n    ('GRU_Attention', create_gru_attention_model),\n]\n\nmodel_scores = {}\nall_test_probs = []\n\n# Load DeBERTa results\ndeberta_probs = np.load(os.path.join(CONFIG['save_dir'], 'deberta_probs.npy'))\nlabel_classes = np.load(os.path.join(CONFIG['save_dir'], 'label_classes.npy'), allow_pickle=True)\n\nmodel_scores['DeBERTa'] = 0.844  # Update this with actual value\nall_test_probs.append(('DeBERTa', deberta_probs))\n\nfor name, builder in tf_models:\n    print(f\"\\n--- Training {name} ---\")\n    \n    # Clear session before each model\n    keras.backend.clear_session()\n    \n    model = builder(vocab_size)\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['tf_learning_rate']),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            filepath=os.path.join(CONFIG['save_dir'], f'{name}_best.weights.h5'),\n            monitor='val_loss',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=2,\n            min_lr=1e-7,\n            verbose=1\n        ),\n        BalancedAccuracyCallback(val_ds)\n    ]\n    \n    history = model.fit(\n        train_ds,\n        epochs=CONFIG['tf_epochs'],\n        validation_data=val_ds,\n        class_weight=tf_class_weights_dict,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    y_pred = np.argmax(model.predict(X_val_vec), axis=1)\n    ba = balanced_accuracy_score(y_val, y_pred)\n    model_scores[name] = ba\n    print(f\"\\n✓ {name} Val BA: {ba:.4f}\")\n    \n    test_probs = model.predict(X_test_vec)\n    all_test_probs.append((name, test_probs))\n    \n    test_pred_labels = [IDX_TO_LABEL[i] for i in np.argmax(test_probs, axis=1)]\n    sub_df = pd.DataFrame({'id': test_df['id'], 'review': test_pred_labels})\n    sub_df.to_csv(f'submission_{name}.csv', index=False)\n    \n    # Clear memory\n    del model\n    keras.backend.clear_session()\n    import gc\n    gc.collect()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"CREATING WEIGHTED ENSEMBLE\")\nprint(\"=\" * 60)\n\ntotal_weight = sum(model_scores.values())\nensemble_probs = np.zeros((len(test_df), NUM_CLASSES))\n\nprint(\"\\nModel weights:\")\nfor name, probs in all_test_probs:\n    weight = model_scores[name] / total_weight\n    ensemble_probs += probs * weight\n    print(f\"  {name}: {weight:.3f} (BA: {model_scores[name]:.4f})\")\n\nensemble_preds = np.argmax(ensemble_probs, axis=1)\nensemble_labels = le.inverse_transform(ensemble_preds)\n\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'review': ensemble_labels\n})\nsubmission.to_csv('submission_ensemble.csv', index=False)\n\ndeberta_preds = np.argmax(deberta_probs, axis=1)\ndeberta_labels = le.inverse_transform(deberta_preds)\nsub_deberta = pd.DataFrame({'id': test_df['id'], 'review': deberta_labels})\nsub_deberta.to_csv('submission_deberta.csv', index=False)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\" * 60)\n\nprint(\"\\nModel Balanced Accuracy Scores:\")\nfor name, score in sorted(model_scores.items(), key=lambda x: -x[1]):\n    print(f\"  {name}: {score:.4f}\")\n\nprint(\"\\nSubmission files created:\")\nprint(\"  ⭐ submission_ensemble.csv (BEST)\")\nprint(\"  ⭐ submission_deberta.csv (DeBERTa only)\")\nprint(\"  - submission_Transformer.csv\")\nprint(\"  - submission_BiLSTM.csv\")\nprint(\"  - submission_CNN_BiLSTM.csv\")\nprint(\"  - submission_GRU_Attention.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T11:04:21.150186Z","iopub.execute_input":"2025-12-13T11:04:21.150914Z","iopub.status.idle":"2025-12-13T11:10:52.765295Z","shell.execute_reply.started":"2025-12-13T11:04:21.150889Z","shell.execute_reply":"2025-12-13T11:10:52.764387Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.18.0\n================================================================================\nTRAINING TensorFlow MODELS\n================================================================================\n\nCreating text vectorization...\nVocabulary size: 29565\n\n--- Training Transformer ---\nEpoch 1/30\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1976 - loss: 2.3914\nEpoch 1: val_loss improved from inf to 3.75069, saving model to /kaggle/working/Transformer_best.weights.h5\n - val_balanced_accuracy: 0.2000 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.1976 - loss: 2.3910 - val_accuracy: 0.0752 - val_loss: 3.7507 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2000\nEpoch 2/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2046 - loss: 1.8409\nEpoch 2: val_loss improved from 3.75069 to 3.22452, saving model to /kaggle/working/Transformer_best.weights.h5\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.2046 - loss: 1.8408 - val_accuracy: 0.3333 - val_loss: 3.2245 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2000\nEpoch 3/30\n\u001b[1m369/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1942 - loss: 1.7392\nEpoch 3: val_loss improved from 3.22452 to 2.52627, saving model to /kaggle/working/Transformer_best.weights.h5\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.1942 - loss: 1.7390 - val_accuracy: 0.3524 - val_loss: 2.5263 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2000\nEpoch 4/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1857 - loss: 1.6741\nEpoch 4: val_loss improved from 2.52627 to 2.40542, saving model to /kaggle/working/Transformer_best.weights.h5\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1857 - loss: 1.6741 - val_accuracy: 0.3333 - val_loss: 2.4054 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2000\nEpoch 5/30\n\u001b[1m369/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1810 - loss: 1.6499\nEpoch 5: val_loss improved from 2.40542 to 2.21592, saving model to /kaggle/working/Transformer_best.weights.h5\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.1810 - loss: 1.6499 - val_accuracy: 0.0924 - val_loss: 2.2159 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2000\nEpoch 6/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1780 - loss: 1.6119\nEpoch 6: val_loss improved from 2.21592 to 1.66975, saving model to /kaggle/working/Transformer_best.weights.h5\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1780 - loss: 1.6121 - val_accuracy: 0.3333 - val_loss: 1.6697 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2000\nEpoch 7/30\n\u001b[1m369/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1879 - loss: 1.6220\nEpoch 7: val_loss did not improve from 1.66975\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1878 - loss: 1.6221 - val_accuracy: 0.0924 - val_loss: 4.0746 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2000\nEpoch 8/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1850 - loss: 1.6289\nEpoch 8: val_loss did not improve from 1.66975\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1850 - loss: 1.6289 - val_accuracy: 0.0924 - val_loss: 4.8799 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2000\nEpoch 9/30\n\u001b[1m369/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1874 - loss: 1.6266\nEpoch 9: val_loss did not improve from 1.66975\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1874 - loss: 1.6265 - val_accuracy: 0.0752 - val_loss: 2.3913 - learning_rate: 5.0000e-04 - val_balanced_accuracy: 0.2000\nEpoch 10/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1821 - loss: 1.6401\nEpoch 10: val_loss did not improve from 1.66975\n\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1821 - loss: 1.6400 - val_accuracy: 0.0924 - val_loss: 2.9119 - learning_rate: 5.0000e-04 - val_balanced_accuracy: 0.2000\nEpoch 11/30\n\u001b[1m369/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.1600 - loss: 1.6103\nEpoch 11: val_loss did not improve from 1.66975\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1600 - loss: 1.6104 - val_accuracy: 0.0924 - val_loss: 2.2453 - learning_rate: 2.5000e-04 - val_balanced_accuracy: 0.2000\nEpoch 11: early stopping\nRestoring model weights from the end of the best epoch: 6.\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\n✓ Transformer Val BA: 0.2000\n\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n\n--- Training BiLSTM ---\nEpoch 1/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3055 - loss: 1.5081\nEpoch 1: val_loss improved from inf to 1.44347, saving model to /kaggle/working/BiLSTM_best.weights.h5\n - val_balanced_accuracy: 0.3438 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.3058 - loss: 1.5077 - val_accuracy: 0.3457 - val_loss: 1.4435 - learning_rate: 0.0010 - val_balanced_accuracy: 0.3438\nEpoch 2/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5355 - loss: 1.0425\nEpoch 2: val_loss did not improve from 1.44347\n - val_balanced_accuracy: 0.4085 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - accuracy: 0.5355 - loss: 1.0426 - val_accuracy: 0.3486 - val_loss: 1.4696 - learning_rate: 0.0010 - val_balanced_accuracy: 0.4085\nEpoch 3/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6383 - loss: 0.7904\nEpoch 3: val_loss improved from 1.44347 to 1.32336, saving model to /kaggle/working/BiLSTM_best.weights.h5\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 37ms/step - accuracy: 0.6382 - loss: 0.7904 - val_accuracy: 0.4790 - val_loss: 1.3234 - learning_rate: 0.0010 - val_balanced_accuracy: 0.3821\nEpoch 4/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7396 - loss: 0.5449\nEpoch 4: val_loss did not improve from 1.32336\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 37ms/step - accuracy: 0.7395 - loss: 0.5451 - val_accuracy: 0.4486 - val_loss: 1.6634 - learning_rate: 0.0010 - val_balanced_accuracy: 0.3898\nEpoch 5/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8046 - loss: 0.4640\nEpoch 5: val_loss did not improve from 1.32336\n\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n - val_balanced_accuracy: 0.4120 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - accuracy: 0.8046 - loss: 0.4641 - val_accuracy: 0.4905 - val_loss: 1.6134 - learning_rate: 0.0010 - val_balanced_accuracy: 0.4120\nEpoch 6/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8716 - loss: 0.2886\nEpoch 6: val_loss did not improve from 1.32336\n - val_balanced_accuracy: 0.4231 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.8717 - loss: 0.2885 - val_accuracy: 0.4629 - val_loss: 2.0146 - learning_rate: 5.0000e-04 - val_balanced_accuracy: 0.4231\nEpoch 7/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9308 - loss: 0.1429\nEpoch 7: val_loss did not improve from 1.32336\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n - val_balanced_accuracy: 0.4418 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.9307 - loss: 0.1430 - val_accuracy: 0.4705 - val_loss: 2.2026 - learning_rate: 5.0000e-04 - val_balanced_accuracy: 0.4418\nEpoch 8/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9481 - loss: 0.1273\nEpoch 8: val_loss did not improve from 1.32336\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 35ms/step - accuracy: 0.9481 - loss: 0.1272 - val_accuracy: 0.4752 - val_loss: 2.4282 - learning_rate: 2.5000e-04 - val_balanced_accuracy: 0.4359\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n\n✓ BiLSTM Val BA: 0.3914\n\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n\n--- Training CNN_BiLSTM ---\nEpoch 1/30\n\u001b[1m370/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2358 - loss: 1.9435\nEpoch 1: val_loss improved from inf to 2.02714, saving model to /kaggle/working/CNN_BiLSTM_best.weights.h5\n - val_balanced_accuracy: 0.2782 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 29ms/step - accuracy: 0.2360 - loss: 1.9423 - val_accuracy: 0.2000 - val_loss: 2.0271 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2782\nEpoch 2/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4046 - loss: 1.3169\nEpoch 2: val_loss improved from 2.02714 to 1.88505, saving model to /kaggle/working/CNN_BiLSTM_best.weights.h5\n - val_balanced_accuracy: 0.3121 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.4046 - loss: 1.3169 - val_accuracy: 0.2781 - val_loss: 1.8851 - learning_rate: 0.0010 - val_balanced_accuracy: 0.3121\nEpoch 3/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5552 - loss: 0.9202\nEpoch 3: val_loss improved from 1.88505 to 1.35770, saving model to /kaggle/working/CNN_BiLSTM_best.weights.h5\n - val_balanced_accuracy: 0.3957 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.5552 - loss: 0.9201 - val_accuracy: 0.4524 - val_loss: 1.3577 - learning_rate: 0.0010 - val_balanced_accuracy: 0.3957\nEpoch 4/30\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6903 - loss: 0.5950\nEpoch 4: val_loss did not improve from 1.35770\n - val_balanced_accuracy: 0.4431 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.6903 - loss: 0.5950 - val_accuracy: 0.4752 - val_loss: 1.3947 - learning_rate: 0.0010 - val_balanced_accuracy: 0.4431\nEpoch 5/30\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7924 - loss: 0.4245\nEpoch 5: val_loss did not improve from 1.35770\n\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.7924 - loss: 0.4245 - val_accuracy: 0.4505 - val_loss: 1.9722 - learning_rate: 0.0010 - val_balanced_accuracy: 0.3113\nEpoch 6/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8520 - loss: 0.3153\nEpoch 6: val_loss did not improve from 1.35770\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.8520 - loss: 0.3152 - val_accuracy: 0.4771 - val_loss: 1.6825 - learning_rate: 5.0000e-04 - val_balanced_accuracy: 0.3909\nEpoch 7/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9200 - loss: 0.1691\nEpoch 7: val_loss did not improve from 1.35770\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n - val_balanced_accuracy: 0.4495 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9200 - loss: 0.1691 - val_accuracy: 0.4305 - val_loss: 1.9732 - learning_rate: 5.0000e-04 - val_balanced_accuracy: 0.4495\nEpoch 8/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9472 - loss: 0.1083\nEpoch 8: val_loss did not improve from 1.35770\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9472 - loss: 0.1082 - val_accuracy: 0.4933 - val_loss: 1.7414 - learning_rate: 2.5000e-04 - val_balanced_accuracy: 0.4479\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n\n✓ CNN_BiLSTM Val BA: 0.3957\n\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n\n--- Training GRU_Attention ---\nEpoch 1/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2870 - loss: 1.5163\nEpoch 1: val_loss improved from inf to 1.39068, saving model to /kaggle/working/GRU_Attention_best.weights.h5\n - val_balanced_accuracy: 0.2860 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.2873 - loss: 1.5158 - val_accuracy: 0.3610 - val_loss: 1.3907 - learning_rate: 0.0010 - val_balanced_accuracy: 0.2860\nEpoch 2/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5207 - loss: 1.0511\nEpoch 2: val_loss improved from 1.39068 to 1.23197, saving model to /kaggle/working/GRU_Attention_best.weights.h5\n - val_balanced_accuracy: 0.3820 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 35ms/step - accuracy: 0.5207 - loss: 1.0512 - val_accuracy: 0.4648 - val_loss: 1.2320 - learning_rate: 0.0010 - val_balanced_accuracy: 0.3820\nEpoch 3/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6606 - loss: 0.7375\nEpoch 3: val_loss did not improve from 1.23197\n - val_balanced_accuracy: 0.4117 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.6606 - loss: 0.7377 - val_accuracy: 0.4867 - val_loss: 1.4735 - learning_rate: 0.0010 - val_balanced_accuracy: 0.4117\nEpoch 4/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7715 - loss: 0.5179\nEpoch 4: val_loss did not improve from 1.23197\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n - val_balanced_accuracy: 0.4396 (best)\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.7715 - loss: 0.5180 - val_accuracy: 0.4581 - val_loss: 1.7940 - learning_rate: 0.0010 - val_balanced_accuracy: 0.4396\nEpoch 5/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8598 - loss: 0.3159\nEpoch 5: val_loss did not improve from 1.23197\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 35ms/step - accuracy: 0.8598 - loss: 0.3158 - val_accuracy: 0.4752 - val_loss: 1.9546 - learning_rate: 5.0000e-04 - val_balanced_accuracy: 0.4005\nEpoch 6/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9147 - loss: 0.1829\nEpoch 6: val_loss did not improve from 1.23197\n\nEpoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 35ms/step - accuracy: 0.9146 - loss: 0.1830 - val_accuracy: 0.5048 - val_loss: 2.0763 - learning_rate: 5.0000e-04 - val_balanced_accuracy: 0.4313\nEpoch 7/30\n\u001b[1m371/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9369 - loss: 0.1350\nEpoch 7: val_loss did not improve from 1.23197\n\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - accuracy: 0.9370 - loss: 0.1349 - val_accuracy: 0.4962 - val_loss: 2.1193 - learning_rate: 2.5000e-04 - val_balanced_accuracy: 0.4346\nEpoch 7: early stopping\nRestoring model weights from the end of the best epoch: 2.\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n\n✓ GRU_Attention Val BA: 0.3937\n\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n\n============================================================\nCREATING WEIGHTED ENSEMBLE\n============================================================\n\nModel weights:\n  DeBERTa: 0.379 (BA: 0.8440)\n  Transformer: 0.090 (BA: 0.2000)\n  BiLSTM: 0.176 (BA: 0.3914)\n  CNN_BiLSTM: 0.178 (BA: 0.3957)\n  GRU_Attention: 0.177 (BA: 0.3937)\n\n============================================================\nTRAINING COMPLETE!\n============================================================\n\nModel Balanced Accuracy Scores:\n  DeBERTa: 0.8440\n  CNN_BiLSTM: 0.3957\n  GRU_Attention: 0.3937\n  BiLSTM: 0.3914\n  Transformer: 0.2000\n\nSubmission files created:\n  ⭐ submission_ensemble.csv (BEST)\n  ⭐ submission_deberta.csv (DeBERTa only)\n  - submission_Transformer.csv\n  - submission_BiLSTM.csv\n  - submission_CNN_BiLSTM.csv\n  - submission_GRU_Attention.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import PreTrainedTokenizerFast, DebertaV2Config, DebertaV2Model\nfrom safetensors.torch import load_file\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nCONFIG = {\n    'models_path': '/kaggle/input/final-data',\n    'test_data_path': '/kaggle/input/nn-26-review-sentiment-classification/test.csv',  # Change this to your test data path\n    'output_dir': '/kaggle/working/',\n    'pt_max_len': 384,\n    'pt_batch_size': 8,\n    'tf_max_len': 200,\n    'tf_batch_size': 16,\n    'tf_max_features': 30000,\n    'tf_embedding_dim': 200,\n    'tf_lstm_units': 96,\n    'tf_gru_units': 96,\n    'tf_cnn_filters': 96,\n    'tf_dropout_rate': 0.4,\n    'transformer_heads': 8,\n    'transformer_ff_dim': 256,\n    'transformer_blocks': 2,\n}\n\nCLASS_LABELS = ['Very bad', 'Bad', 'Good', 'Very good', 'Excellent']\nNUM_CLASSES = len(CLASS_LABELS)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Models path: {CONFIG['models_path']}\")\nprint(f\"Test data path: {CONFIG['test_data_path']}\")\n\n# ============================================================================\n# HELPER FUNCTIONS\n# ============================================================================\ndef clean_text(text):\n    if pd.isna(text):\n        return \"\"\n    text = str(text)\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'<.*?>', '', text)\n    contractions = {\n        \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\",\n        \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n    }\n    for contraction, expansion in contractions.items():\n        text = text.replace(contraction, expansion)\n    text = re.sub(r'[^a-zA-Z0-9\\s!?.,;:\\'-]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# ============================================================================\n# PYTORCH MODEL DEFINITIONS\n# ============================================================================\nclass DeBERTaClassifier(nn.Module):\n    def __init__(self, num_classes, dropout=0.3):\n        super().__init__()\n        \n        self.config = DebertaV2Config(\n            architectures=[\"DebertaV2Model\"],\n            attention_probs_dropout_prob=0.1,\n            hidden_act=\"gelu\",\n            hidden_dropout_prob=0.1,\n            hidden_size=1024,\n            initializer_range=0.02,\n            intermediate_size=4096,\n            layer_norm_eps=1e-7,\n            max_position_embeddings=512,\n            max_relative_positions=-1,\n            model_type=\"deberta-v2\",\n            norm_rel_ebd=\"layer_norm\",\n            num_attention_heads=16,\n            num_hidden_layers=24,\n            pad_token_id=0,\n            pooler_dropout=0,\n            pooler_hidden_act=\"gelu\",\n            pooler_hidden_size=1024,\n            pos_att_type=[\"p2c\", \"c2p\"],\n            position_biased_input=False,\n            position_buckets=256,\n            relative_attention=True,\n            share_att_key=True,\n            type_vocab_size=0,\n            vocab_size=128100,\n            torch_dtype=\"float32\"\n        )\n        \n        self.transformer = DebertaV2Model(self.config)\n        hidden_size = self.config.hidden_size\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for _ in range(5)])\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, num_classes)\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n            pooled = outputs.pooler_output\n        else:\n            last_hidden = outputs.last_hidden_state\n            attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n            sum_hidden = torch.sum(last_hidden * attention_mask_expanded, 1)\n            sum_mask = attention_mask_expanded.sum(1).clamp(min=1e-9)\n            pooled = sum_hidden / sum_mask\n        \n        pooled = self.layer_norm(pooled)\n        \n        logits = torch.zeros(pooled.size(0), self.classifier[-1].out_features).to(pooled.device)\n        for dropout in self.dropouts:\n            logits += self.classifier(dropout(pooled))\n        logits /= len(self.dropouts)\n        \n        return logits\n\n# ============================================================================\n# TENSORFLOW CUSTOM LAYERS\n# ============================================================================\nclass BahdanauAttention(layers.Layer):\n    def __init__(self, units, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.W1 = layers.Dense(units)\n        self.W2 = layers.Dense(units)\n        self.V = layers.Dense(1)\n    \n    def call(self, query, values):\n        query_with_time_axis = tf.expand_dims(query, 1)\n        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\"units\": self.units})\n        return config\n\nclass PositionalEncoding(layers.Layer):\n    def __init__(self, max_len, d_model, **kwargs):\n        super().__init__(**kwargs)\n        self.max_len = max_len\n        self.d_model = d_model\n        \n        position = np.arange(max_len)[:, np.newaxis]\n        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n        \n        pos_encoding = np.zeros((max_len, d_model))\n        pos_encoding[:, 0::2] = np.sin(position * div_term)\n        pos_encoding[:, 1::2] = np.cos(position * div_term)\n        \n        self.pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n    \n    def call(self, x):\n        seq_len = tf.shape(x)[1]\n        return x + self.pos_encoding[:seq_len, :]\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\"max_len\": self.max_len, \"d_model\": self.d_model})\n        return config\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.ff_dim = ff_dim\n        self.dropout_rate = dropout_rate\n        \n        self.att = layers.MultiHeadAttention(\n            num_heads=num_heads, \n            key_dim=embed_dim // num_heads,\n            dropout=dropout_rate\n        )\n        self.ffn = keras.Sequential([\n            layers.Dense(ff_dim, activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dim),\n        ])\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(dropout_rate)\n        self.dropout2 = layers.Dropout(dropout_rate)\n    \n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs, training=training)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        \n        ffn_output = self.ffn(out1, training=training)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n        \n        return out2\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"ff_dim\": self.ff_dim,\n            \"dropout_rate\": self.dropout_rate\n        })\n        return config\n\n# ============================================================================\n# TENSORFLOW MODEL BUILDERS\n# ============================================================================\ndef create_transformer_model(vocab_size, num_classes=5):\n    inputs = layers.Input(shape=(CONFIG['tf_max_len'],))\n    x = layers.Embedding(vocab_size, CONFIG['tf_embedding_dim'])(inputs)\n    x = layers.SpatialDropout1D(0.2)(x)\n    x = PositionalEncoding(CONFIG['tf_max_len'], CONFIG['tf_embedding_dim'])(x)\n    \n    for _ in range(CONFIG['transformer_blocks']):\n        x = TransformerBlock(\n            embed_dim=CONFIG['tf_embedding_dim'],\n            num_heads=CONFIG['transformer_heads'],\n            ff_dim=CONFIG['transformer_ff_dim'],\n            dropout_rate=CONFIG['tf_dropout_rate'] * 0.5\n        )(x)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool, max_pool])\n    \n    x = layers.Dense(256, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    x = layers.Dense(128, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    \n    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    return keras.Model(inputs=inputs, outputs=outputs, name='Transformer')\n\ndef create_bilstm_model(vocab_size, num_classes=5):\n    inputs = layers.Input(shape=(CONFIG['tf_max_len'],))\n    x = layers.Embedding(vocab_size, CONFIG['tf_embedding_dim'], mask_zero=True)(inputs)\n    x = layers.SpatialDropout1D(0.2)(x)\n    \n    x = layers.Bidirectional(\n        layers.LSTM(CONFIG['tf_lstm_units'], return_sequences=True, dropout=0.2, recurrent_dropout=0)\n    )(x)\n    x = layers.Bidirectional(\n        layers.LSTM(CONFIG['tf_lstm_units'] // 2, return_sequences=True, dropout=0.2, recurrent_dropout=0)\n    )(x)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    x = layers.concatenate([avg_pool, max_pool])\n    \n    x = layers.Dense(256, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    x = layers.Dense(128, activation='gelu')(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    \n    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    return keras.Model(inputs=inputs, outputs=outputs, name='BiLSTM')\n\ndef create_cnn_bilstm_model(vocab_size, num_classes=5):\n    inputs = layers.Input(shape=(CONFIG['tf_max_len'],))\n    x = layers.Embedding(vocab_size, CONFIG['tf_embedding_dim'])(inputs)\n    x = layers.SpatialDropout1D(0.2)(x)\n    \n    conv_blocks = []\n    for kernel_size in [2, 3, 4]:\n        conv = layers.Conv1D(CONFIG['tf_cnn_filters'], kernel_size, activation='relu', padding='same')(x)\n        conv = layers.BatchNormalization()(conv)\n        conv = layers.GlobalMaxPooling1D()(conv)\n        conv_blocks.append(conv)\n    \n    cnn_features = layers.concatenate(conv_blocks)\n    \n    lstm_out = layers.Bidirectional(\n        layers.LSTM(CONFIG['tf_lstm_units'], return_sequences=True, recurrent_dropout=0)\n    )(x)\n    lstm_out = layers.GlobalAveragePooling1D()(lstm_out)\n    \n    x = layers.concatenate([cnn_features, lstm_out])\n    \n    x = layers.Dense(256, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    x = layers.Dense(128, activation='gelu')(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    \n    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    return keras.Model(inputs=inputs, outputs=outputs, name='CNN_BiLSTM')\n\ndef create_gru_attention_model(vocab_size, num_classes=5):\n    inputs = layers.Input(shape=(CONFIG['tf_max_len'],))\n    x = layers.Embedding(vocab_size, CONFIG['tf_embedding_dim'], mask_zero=True)(inputs)\n    x = layers.SpatialDropout1D(0.2)(x)\n    \n    x = layers.Bidirectional(\n        layers.GRU(CONFIG['tf_gru_units'], return_sequences=True, dropout=0.2, recurrent_dropout=0)\n    )(x)\n    gru_out = layers.Bidirectional(\n        layers.GRU(CONFIG['tf_gru_units'] // 2, return_sequences=True, dropout=0.2, recurrent_dropout=0)\n    )(x)\n    \n    query = layers.Lambda(lambda x: x[:, -1, :])(gru_out)\n    context_vector, _ = BahdanauAttention(CONFIG['tf_gru_units'])(query, gru_out)\n    \n    avg_pool = layers.GlobalAveragePooling1D()(gru_out)\n    max_pool = layers.GlobalMaxPooling1D()(gru_out)\n    \n    x = layers.concatenate([context_vector, avg_pool, max_pool])\n    \n    x = layers.Dense(256, activation='gelu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(CONFIG['tf_dropout_rate'])(x)\n    x = layers.Dense(128, activation='gelu')(x)\n    \n    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    return keras.Model(inputs=inputs, outputs=outputs, name='GRU_Attention')\n\n# ============================================================================\n# LOAD DATA\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING TEST DATA\")\nprint(\"=\"*80)\n\ntest_df = pd.read_csv(CONFIG['test_data_path'])\nprint(f\"Test samples: {len(test_df)}\")\n\ntest_df['cleaned_text'] = test_df['text'].apply(clean_text)\n\n# ============================================================================\n# LOAD DEBERTA MODEL AND PREDICT\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING DeBERTa MODEL\")\nprint(\"=\"*80)\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file=os.path.join(CONFIG['models_path'], 'tokenizer.json')\n)\n\n# Set pad token if not present\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    \nprint(f\"Tokenizer vocab size: {len(tokenizer)}\")\nprint(f\"Pad token: {tokenizer.pad_token}\")\n\nmodel_deberta = DeBERTaClassifier(NUM_CLASSES, dropout=0.3).to(DEVICE)\nmodel_deberta.load_state_dict(\n    torch.load(os.path.join(CONFIG['models_path'], 'best_deberta.pt'), \n               map_location=DEVICE)\n)\nmodel_deberta.eval()\n\nprint(\"Making DeBERTa predictions...\")\ndeberta_probs = []\nwith torch.no_grad():\n    for i in range(0, len(test_df), CONFIG['pt_batch_size']):\n        batch_texts = test_df['cleaned_text'].iloc[i:i+CONFIG['pt_batch_size']].tolist()\n        \n        encodings = tokenizer.batch_encode_plus(\n            batch_texts,\n            add_special_tokens=True,\n            max_length=CONFIG['pt_max_len'],\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encodings['input_ids'].to(DEVICE)\n        attention_mask = encodings['attention_mask'].to(DEVICE)\n        \n        outputs = model_deberta(input_ids, attention_mask)\n        probs = F.softmax(outputs, dim=1)\n        deberta_probs.extend(probs.cpu().numpy())\n\ndeberta_probs = np.array(deberta_probs)\nprint(f\"✓ DeBERTa predictions shape: {deberta_probs.shape}\")\n\n# Save DeBERTa predictions\ndeberta_preds = np.argmax(deberta_probs, axis=1)\ndeberta_labels = [CLASS_LABELS[i] for i in deberta_preds]\nsubmission_deberta = pd.DataFrame({'id': test_df['id'], 'review': deberta_labels})\nsubmission_deberta.to_csv(os.path.join(CONFIG['output_dir'], 'submission_deberta.csv'), index=False)\nprint(\"✓ Saved: submission_deberta.csv\")\n\n# Clear GPU memory\ndel model_deberta\ntorch.cuda.empty_cache()\n\n# ============================================================================\n# LOAD TENSORFLOW CONFIG AND VECTORIZER\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING TensorFlow CONFIGURATION\")\nprint(\"=\"*80)\n\nwith open(os.path.join(CONFIG['models_path'], 'tf_config.pkl'), 'rb') as f:\n    tf_config = pickle.load(f)\n\nvocab = tf_config['vocab']\nvocab_size = len(vocab)\nprint(f\"Vocabulary size: {vocab_size}\")\n\n# Create and adapt vectorizer\nvectorize_layer = layers.TextVectorization(\n    max_tokens=CONFIG['tf_max_features'],\n    output_mode='int',\n    output_sequence_length=CONFIG['tf_max_len'],\n    vocabulary=vocab\n)\n\nX_test_vec = vectorize_layer(test_df['cleaned_text'].values).numpy()\nprint(f\"✓ Vectorized test data shape: {X_test_vec.shape}\")\n\n# ============================================================================\n# LOAD AND PREDICT WITH TENSORFLOW MODELS\n# ============================================================================\ntf_models_info = [\n    ('Transformer', create_transformer_model),\n    ('BiLSTM', create_bilstm_model),\n    ('CNN_BiLSTM', create_cnn_bilstm_model),\n    ('GRU_Attention', create_gru_attention_model),\n]\n\nall_tf_probs = {}\n\nfor name, builder in tf_models_info:\n    print(f\"\\n--- Loading {name} ---\")\n    keras.backend.clear_session()\n    \n    model = builder(vocab_size)\n    model.load_weights(os.path.join(CONFIG['models_path'], f'{name}_best.weights.h5'))\n    \n    print(f\"Making {name} predictions...\")\n    probs = model.predict(X_test_vec, batch_size=CONFIG['tf_batch_size'], verbose=0)\n    all_tf_probs[name] = probs\n    \n    # Save individual predictions\n    preds = np.argmax(probs, axis=1)\n    pred_labels = [CLASS_LABELS[i] for i in preds]\n    submission = pd.DataFrame({'id': test_df['id'], 'review': pred_labels})\n    submission.to_csv(os.path.join(CONFIG['output_dir'], f'submission_{name}.csv'), index=False)\n    print(f\"✓ Saved: submission_{name}.csv\")\n    \n    del model\n    keras.backend.clear_session()\n\n# ============================================================================\n# CREATE ENSEMBLE PREDICTIONS\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING ENSEMBLE PREDICTIONS\")\nprint(\"=\"*80)\n\n# Ensemble 1: TensorFlow models only\nprint(\"\\n1. TensorFlow Models Ensemble\")\ntf_ensemble_probs = np.mean([all_tf_probs[name] for name in all_tf_probs.keys()], axis=0)\ntf_preds = np.argmax(tf_ensemble_probs, axis=1)\ntf_labels = [CLASS_LABELS[i] for i in tf_preds]\nsubmission = pd.DataFrame({'id': test_df['id'], 'review': tf_labels})\nsubmission.to_csv(os.path.join(CONFIG['output_dir'], 'submission_tf_ensemble.csv'), index=False)\nprint(\"✓ Saved: submission_tf_ensemble.csv\")\n\n# Ensemble 2: TensorFlow + DeBERTa (weighted by validation performance)\nprint(\"\\n2. TensorFlow + DeBERTa Ensemble\")\n# Approximate weights based on validation scores\nweights = {\n    'DeBERTa': 0.844,\n    'Transformer': 0.200,\n    'BiLSTM': 0.391,\n    'CNN_BiLSTM': 0.396,\n    'GRU_Attention': 0.394\n}\ntotal_weight = sum(weights.values())\nnormalized_weights = {k: v/total_weight for k, v in weights.items()}\n\ntf_deberta_ensemble = normalized_weights['DeBERTa'] * deberta_probs\nfor name in all_tf_probs.keys():\n    tf_deberta_ensemble += normalized_weights[name] * all_tf_probs[name]\n\ntf_deberta_preds = np.argmax(tf_deberta_ensemble, axis=1)\ntf_deberta_labels = [CLASS_LABELS[i] for i in tf_deberta_preds]\nsubmission = pd.DataFrame({'id': test_df['id'], 'review': tf_deberta_labels})\nsubmission.to_csv(os.path.join(CONFIG['output_dir'], 'submission_tf_deberta_ensemble.csv'), index=False)\nprint(\"✓ Saved: submission_tf_deberta_ensemble.csv\")\n\nprint(\"\\nWeights used:\")\nfor name, weight in normalized_weights.items():\n    print(f\"  {name}: {weight:.3f}\")\n\n# Ensemble 3: TensorFlow + Custom Transformer (weighted)\nprint(\"\\n3. TensorFlow + Custom Transformer Ensemble\")\ntf_custom_transformer_weights = {\n    'Transformer': 0.200,\n    'BiLSTM': 0.391,\n    'CNN_BiLSTM': 0.396,\n    'GRU_Attention': 0.394\n}\ntotal_weight = sum(tf_custom_transformer_weights.values())\nnormalized_weights = {k: v/total_weight for k, v in tf_custom_transformer_weights.items()}\n\ntf_custom_ensemble = np.zeros((len(test_df), NUM_CLASSES))\nfor name in tf_custom_transformer_weights.keys():\n    tf_custom_ensemble += normalized_weights[name] * all_tf_probs[name]\n\ntf_custom_preds = np.argmax(tf_custom_ensemble, axis=1)\ntf_custom_labels = [CLASS_LABELS[i] for i in tf_custom_preds]\nsubmission = pd.DataFrame({'id': test_df['id'], 'review': tf_custom_labels})\nsubmission.to_csv(os.path.join(CONFIG['output_dir'], 'submission_tf_custom_transformer.csv'), index=False)\nprint(\"✓ Saved: submission_tf_custom_transformer.csv\")\n\n# Ensemble 4: All models (equal weight)\nprint(\"\\n4. All Models Equal Weight Ensemble\")\nall_models_equal = deberta_probs.copy()\nfor name in all_tf_probs.keys():\n    all_models_equal += all_tf_probs[name]\nall_models_equal /= (len(all_tf_probs) + 1)\n\nall_equal_preds = np.argmax(all_models_equal, axis=1)\nall_equal_labels = [CLASS_LABELS[i] for i in all_equal_preds]\nsubmission = pd.DataFrame({'id': test_df['id'], 'review': all_equal_labels})\nsubmission.to_csv(os.path.join(CONFIG['output_dir'], 'submission_all_models_equal.csv'), index=False)\nprint(\"✓ Saved: submission_all_models_equal.csv\")\n\n# Ensemble 5: All Transformers (DeBERTa + Custom Transformer)\nprint(\"\\n5. All Transformers Ensemble\")\ntransformer_weights = {\n    'DeBERTa': 0.844,\n    'Transformer': 0.200\n}\ntotal_weight = sum(transformer_weights.values())\nnormalized_weights = {k: v/total_weight for k, v in transformer_weights.items()}\n\nall_transformers_ensemble = (normalized_weights['DeBERTa'] * deberta_probs + \n                              normalized_weights['Transformer'] * all_tf_probs['Transformer'])\n\nall_transformers_preds = np.argmax(all_transformers_ensemble, axis=1)\nall_transformers_labels = [CLASS_LABELS[i] for i in all_transformers_preds]\nsubmission = pd.DataFrame({'id': test_df['id'], 'review': all_transformers_labels})\nsubmission.to_csv(os.path.join(CONFIG['output_dir'], 'submission_all_transformers.csv'), index=False)\nprint(\"✓ Saved: submission_all_transformers.csv\")\n\n# ============================================================================\n# SUMMARY\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"INFERENCE COMPLETE!\")\nprint(\"=\"*80)\n\nprint(\"\\nGenerated submission files:\")\nprint(\"\\nIndividual Models:\")\nprint(\"  - submission_deberta.csv (DeBERTa)\")\nprint(\"  - submission_Transformer.csv (Custom Transformer)\")\nprint(\"  - submission_BiLSTM.csv\")\nprint(\"  - submission_CNN_BiLSTM.csv\")\nprint(\"  - submission_GRU_Attention.csv\")\n\nprint(\"\\nEnsemble Combinations:\")\nprint(\"  ⭐ submission_tf_deberta_ensemble.csv (TensorFlow + DeBERTa, weighted - RECOMMENDED)\")\nprint(\"  - submission_tf_ensemble.csv (TensorFlow models only)\")\nprint(\"  - submission_tf_custom_transformer.csv (TensorFlow + Custom Transformer)\")\nprint(\"  - submission_all_models_equal.csv (All models, equal weight)\")\nprint(\"  - submission_all_transformers.csv (DeBERTa + Custom Transformer)\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T11:21:30.098516Z","iopub.execute_input":"2025-12-13T11:21:30.099258Z","iopub.status.idle":"2025-12-13T11:24:35.280688Z","shell.execute_reply.started":"2025-12-13T11:21:30.099232Z","shell.execute_reply":"2025-12-13T11:24:35.279857Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nModels path: /kaggle/input/final-data\nTest data path: /kaggle/input/nn-26-review-sentiment-classification/test.csv\n\n================================================================================\nLOADING TEST DATA\n================================================================================\nTest samples: 3000\n\n================================================================================\nLOADING DeBERTa MODEL\n================================================================================\nTokenizer vocab size: 128001\nPad token: [PAD]\nMaking DeBERTa predictions...\n✓ DeBERTa predictions shape: (3000, 5)\n✓ Saved: submission_deberta.csv\n\n================================================================================\nLOADING TensorFlow CONFIGURATION\n================================================================================\nVocabulary size: 29565\n✓ Vectorized test data shape: (3000, 200)\n\n--- Loading Transformer ---\nMaking Transformer predictions...\n✓ Saved: submission_Transformer.csv\n\n--- Loading BiLSTM ---\nMaking BiLSTM predictions...\n✓ Saved: submission_BiLSTM.csv\n\n--- Loading CNN_BiLSTM ---\nMaking CNN_BiLSTM predictions...\n✓ Saved: submission_CNN_BiLSTM.csv\n\n--- Loading GRU_Attention ---\nMaking GRU_Attention predictions...\n✓ Saved: submission_GRU_Attention.csv\n\n================================================================================\nCREATING ENSEMBLE PREDICTIONS\n================================================================================\n\n1. TensorFlow Models Ensemble\n✓ Saved: submission_tf_ensemble.csv\n\n2. TensorFlow + DeBERTa Ensemble\n✓ Saved: submission_tf_deberta_ensemble.csv\n\nWeights used:\n  DeBERTa: 0.379\n  Transformer: 0.090\n  BiLSTM: 0.176\n  CNN_BiLSTM: 0.178\n  GRU_Attention: 0.177\n\n3. TensorFlow + Custom Transformer Ensemble\n✓ Saved: submission_tf_custom_transformer.csv\n\n4. All Models Equal Weight Ensemble\n✓ Saved: submission_all_models_equal.csv\n\n5. All Transformers Ensemble\n✓ Saved: submission_all_transformers.csv\n\n================================================================================\nINFERENCE COMPLETE!\n================================================================================\n\nGenerated submission files:\n\nIndividual Models:\n  - submission_deberta.csv (DeBERTa)\n  - submission_Transformer.csv (Custom Transformer)\n  - submission_BiLSTM.csv\n  - submission_CNN_BiLSTM.csv\n  - submission_GRU_Attention.csv\n\nEnsemble Combinations:\n  ⭐ submission_tf_deberta_ensemble.csv (TensorFlow + DeBERTa, weighted - RECOMMENDED)\n  - submission_tf_ensemble.csv (TensorFlow models only)\n  - submission_tf_custom_transformer.csv (TensorFlow + Custom Transformer)\n  - submission_all_models_equal.csv (All models, equal weight)\n  - submission_all_transformers.csv (DeBERTa + Custom Transformer)\n\n================================================================================\n","output_type":"stream"}],"execution_count":11}]}